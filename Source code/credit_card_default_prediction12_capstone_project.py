# -*- coding: utf-8 -*-
"""Credit Card Default Prediction12 - Capstone Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1M3l4jP2lk4NmP3YJSdbb1W4-QIYUXfUY

# <b><u> Project Title : Credit Card Default Prediction 
## **Predicting whether a customer will default on his/her credit card**</b>

# <b> Problem Statement </b>

### This project is aimed at predicting the case of customers default payments in Taiwan. From the perspective of risk management, the result of predictive accuracy of the estimated probability of default will be more valuable than the binary result of classification - credible or not credible clients.

###Can we reliably predict who has is likely to default? If so, the bank may be able to prevent the loss by providing the customer with alternative options (such as forbearance or debt consolidation, etc.). I will use various machine learning classification techniques to perform my analysis.

# <b> Data Description </b>

### <b>Attribute Information: </b>

### This research employed a binary variable, default payment (Yes = 1, No = 0), as the response variable. This study reviewed the literature and used the following 23 variables as explanatory variables:
* ### X1: Amount of the given credit (NT dollar): it includes both the individual consumer credit and his/her family (supplementary) credit.
* ### X2: Gender (1 = male; 2 = female).
* ### X3: Education (1 = graduate school; 2 = university; 3 = high school; 4 = others).
* ### X4: Marital status (1 = married; 2 = single; 3 = others).
* ### X5: Age (year).
* ### X6 - X11: History of past payment. We tracked the past monthly payment records (from April to September, 2005) as follows: X6 = the repayment status in September, 2005; X7 = the repayment status in August, 2005; . . .;X11 = the repayment status in April, 2005. The measurement scale for the repayment status is: -1 = pay duly; 1 = payment delay for one month; 2 = payment delay for two months; . . .; 8 = payment delay for eight months; 9 = payment delay for nine months and above.
* ### X12-X17: Amount of bill statement (NT dollar). X12 = amount of bill statement in September, 2005; X13 = amount of bill statement in August, 2005; . . .; X17 = amount of bill statement in April, 2005.
* ### X18-X23: Amount of previous payment (NT dollar). X18 = amount paid in September, 2005; X19 = amount paid in August, 2005; . . .;X23 = amount paid in April, 2005.

#**Importing Libraries**
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib import pyplot as plt

# Sklearn Libraries
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import  accuracy_score, f1_score, precision_score, recall_score, roc_auc_score
from sklearn.model_selection import GridSearchCV

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier

import warnings
warnings.filterwarnings('ignore')

#Mounting Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Importing the dataset

credit_df = pd.read_csv('/content/drive/MyDrive/Credit Card Default Detection(Supervised ML Classification)-Tito Varghese/default of credit card clients.csv')

"""# **Dataset Inspection**"""

credit_df.head()

credit_df.shape

credit_df.columns

credit_df.info()

"""###**Renaming the Column name**
1.**Renamed the columns using the first record given in the dataset and dropped the first record.**

2.**Renamed our target variable to defaulter and PAY_0 column to PAY_1**

3.**Converted the datatypes of all columns from object to int datatype because all columns contains numerical values.**

4.**Drop the ID column from the dataset,since its not an influential feature in our modeling**
"""

# Renaming the columns in the dataset
credit_df.rename(columns= credit_df.loc[0,:],inplace=True)

# Dropping the first record after renaming the columns
credit_df= credit_df[1:]

#Renaming the column name PAY_0
credit_df.rename(columns={'PAY_0':'PAY_1','default payment next month':'defaulter'},inplace=True)

# Converting the datatype from object to integer in all the columns
credit_df = credit_df.astype(object).astype(int)

credit_df.drop(labels='ID',inplace=True,axis=1)

credit_df.info()

credit_df.describe()

"""###**The  Dataset Inspection Summary**
1. **The average credit card limit/consumer credit amount is 167484.32(NT Dollars)**
2. **The maximum number of credit card holders were females in Taiwan.**
3. **The given dataset consist of 30000 rows and 24 columns**
4. **The feature 'defaulter'  is our dependent/target feature**
5. **The most number of credit card holders were having university degree education.**
6. **The most of the customers marriage status was Single, who carries a credit card.**

#**Thought Process and Methodolgy**

What actually will drive the customer to default the payment?

Do we think of all the given variables when we are trying to find out a defaulter? 
e.g. When we think about the defaulter, do we care about its 'Male or Female'.Do gender has any infulence on our target variable?.

If so, how important would this variable be? 
e.g. What is the impact of having that SEX variable? Whether the variable has a postive or negative correlation with our target variable defaulter.

Is this information already described in any other variable? 

These are few questions you ask before predicting whether the customer will be defaulter or not -

What is the customer past payment history?

What is his credit limit and whether he is paying the bills in a timely manner?

What is his/her age and educational background?

... Lot more..

**The steps followed in this Classification Project were as follows--**


Data Cleaning

Feature Engineering

Exploratory Data Analysis

Handling Class Imbalance

Buliding  Model using different Algorithms 

Evaluation Metrics

Feature Importance/Selection

Hyperparameter Tuning

Analyze Results

#**Data Cleaning**
"""

#check for null values
credit_df.isnull().sum()

#check for duplicate rows
credit_df.duplicated().sum()

#dropping duplicate rows
credit_df = credit_df.drop_duplicates()

credit_df.duplicated().sum()

#Check for imbalance data in Target Feature
(credit_df['defaulter'].value_counts()/len(credit_df['defaulter'])*100)

ax = sns.countplot(x=credit_df['defaulter']);
plt.title('Distribution of defaulter')
for p in ax.patches:
    height = p.get_height()
    ax.text(x = p.get_x()+(p.get_width()/2), # x-coordinate position of data label, padded to be in the middle of the bar
    y = height+0.2, ha = 'center',s = '{:.0f}'.format(height)) # data label, formatted to ignore decimals
    #ha = ‘center’) # sets horizontal alignment (ha) to center
plt.xticks(rotation = 'vertical')    
plt.show()

"""###Distribution of target classes is highly imbalanced, non-defaults far outnumber defaults. This is common in these datasets since most people pay credit cards on time

###**Data Cleaning Summary**
1. **Tha given dataset does not contain any missing values**
2. **The given dataset had 35 duplicate rows and we have dropped the duplicate records.**
3. **The target variable consist of imbalance data with 77.87% Non defaulters(0 value) and 22.12% 1 defaulters (1 value).**

#**Feature Engineering**

1.**Replaced 0 class/category in marriage column into class/category 3='others' because 0 class is not defined in the marriage variable.**

2.**Similarly, we replaced few undefined classes like 0, 5 and 6 in education column into class 4='others' using a fuction name education.** 

3.**Replaced negative values in payment history columns i.e (PAY_1,PAY_2...PAY_6)
into class 0 -pay duly on time.** 

4.**Two new Features were derived from the exisiting independent features,because it will help to train our model more effectively**

###**Replacing few classes in MARRIAGE and EDUCATION variables**
"""

credit_df['MARRIAGE'].unique()

credit_df['EDUCATION'].unique()

credit_df['MARRIAGE'].replace(to_replace=0,value=3,inplace=True)

def education(value):
  if value> 4:
    value = 4
  elif value==0:
    value= 4
  else:
    value
  return value

credit_df['EDUCATION']= credit_df['EDUCATION'].apply(education)

credit_df['MARRIAGE'].value_counts()

credit_df['EDUCATION'].value_counts()

"""###**Replaced negative values in payment history columns**"""

payment_list = ['PAY_1','PAY_2','PAY_3','PAY_4','PAY_5','PAY_6']

for var in payment_list:
  credit_df.loc[(credit_df[var] == -1) | (credit_df[var]==-2),var]=0

credit_df.PAY_1.value_counts()

"""###**New Features were derived from the exisiting features**"""

credit_df['Total_Bill_AMT']=credit_df['BILL_AMT1']	+ credit_df['BILL_AMT2']+ credit_df['BILL_AMT3']	+ credit_df['BILL_AMT4']	+ credit_df['BILL_AMT5']	+ credit_df['BILL_AMT6']

credit_df['Total_Paid_AMT']= credit_df['PAY_AMT1']	+ credit_df['PAY_AMT2']+ credit_df['PAY_AMT3']	+ credit_df['PAY_AMT4']	+ credit_df['PAY_AMT5']	+ credit_df['PAY_AMT6']

credit_df['Pending_Payment_AMT']= credit_df['Total_Bill_AMT']- credit_df['Total_Paid_AMT']

credit_df.columns

"""#**Exploratory Data Analysis and Visualization**

##**UNIVARIATE ANALYSIS**

##MARRIAGE
"""

ax = sns.countplot(x=credit_df['MARRIAGE']);
plt.title('Distribution of marriage')
for p in ax.patches:
    height = p.get_height()
    ax.text(x = p.get_x()+(p.get_width()/2), # x-coordinate position of data label, padded to be in the middle of the bar
    y = height+0.2, ha = 'center',s = '{:.0f}'.format(height)) # data label, formatted to ignore decimals
    #ha = ‘center’) # sets horizontal alignment (ha) to center
plt.xticks(rotation = 'vertical')    
plt.show()

"""**From the above graph,we can conclude that the most number of credit card holders were not married(Single)**

##EDUCATION
"""

ax = sns.countplot(x=credit_df['EDUCATION']);
plt.title('Distribution of education')
for p in ax.patches:
    height = p.get_height()
    ax.text(x = p.get_x()+(p.get_width()/2), # x-coordinate position of data label, padded to be in the middle of the bar
    y = height+0.2, ha = 'center',s = '{:.0f}'.format(height)) # data label, formatted to ignore decimals
    #ha = ‘center’) # sets horizontal alignment (ha) to center
plt.xticks(rotation = 'vertical')    
plt.show()

"""**The above graph conveys us that the most number of customers were holding  a university degree as their educational qualification followed by graduates degree holders**

##SEX
"""

ax = sns.countplot(x=credit_df['SEX']);
plt.title('Distribution of sex')
for p in ax.patches:
    height = p.get_height()
    ax.text(x = p.get_x()+(p.get_width()/2), # x-coordinate position of data label, padded to be in the middle of the bar
    y = height+0.2, ha = 'center',s = '{:.0f}'.format(height)) # data label, formatted to ignore decimals
    #ha = ‘center’) # sets horizontal alignment (ha) to center
plt.xticks(rotation = 'vertical')    
plt.show()

"""**The above figure tells us that the most number of credit card holders in Taiwan were females.** 

**Hence proved the famous saying,the females usually do more shopping compared to mens  from the above given data**

##AGE
"""

figure, ax = plt.subplots(nrows = 1, ncols=1, figsize = (25,15))
sns.countplot(x = 'AGE', data = credit_df)
plt.title('Distribution of Age')

"""**The above figure shows the coutplot of Age column** 

The most frequent age  of  the credit card holder is between 25-34.

The highest proportion of credit card holders were youth in the age of 29. 

Hence we can understand that mostly credit cards were popular among youths of taiwan than the older people.

##Repayment Status (Payment History)
"""

def PayColumnsNames(prefix):
    return [prefix+str(x) for x in range(1,7)]

# PAY_1 , PAY_2 , PAY_3 , PAY_4 , PAY_5, PAY_6
pay_columns = PayColumnsNames('PAY_')
figure, ax = plt.subplots(2,3)
figure.set_size_inches(20,15)


for i in range(len(pay_columns)):
    row,col = int(i/3), i%3

    x  = credit_df[pay_columns[i]].value_counts()
    y = credit_df[pay_columns[i]][(credit_df['defaulter']==1)].value_counts()
    ax[row,col].bar(x.index, x, align='center', color='green')
    ax[row,col].bar(y.index, y, align='center', color='red', alpha=0.7)
    ax[row,col].set_title(pay_columns[i])
   


    
plt.show()

"""**The above figure shows the bar plot of payment history status for past six  months starting from September to April , which show the count of defaulters and non-defaulter**

The payment history status consist of unique values like 
0- No delay in  payment
2- payment delay of 2 months
4 - payment delay of 4 months
6- payment delay of 6 months
8- payment delay of 8 months

The green bins shows the count of payment status of all the customers (both defaulters and non-defaulters). On the otherhand, the red bins shows the count of payment status explicitly for the customers who were  defaulters.

From the above graph, we can conclude that if the payment status is greater than 2 months,then there is a 90% chance of the customer to default the payment.

##Previous Amount Paid (PAY_AMT)
"""

pay_amt_columns = PayColumnsNames('PAY_AMT')
figure, ax = plt.subplots(2,3)
figure.set_size_inches(18,8)


for i in range(len(pay_columns)):
    row,col = int(i/3), i%3

    ax[row,col].hist(credit_df[pay_amt_columns[i]], 30, color ='green')
    ax[row,col].hist(credit_df[pay_amt_columns[i]][(credit_df['defaulter']==1)],30,color='red',alpha = 0.7)
    ax[row,col].set_title(pay_amt_columns[i])
    #adding scaling to make the graph more helpful
    ax[row,col].set_yscale('log', nonposy='clip')


    
plt.tight_layout()
plt.show()

"""**The above histogram shows the distribution of payment amount for each month explicitly for defaulters after scaling the data using log transform** 

The green bins in the histograms shows the payment_amount for all the customers from September to April Month.

The red bins in the histogram tells the payment amount of customers who were actually a defaulter from September to April Month.

We can tell from the above histogram that the payment amount of defaulters were relatively very low compared to non defaulters and the defaulters payment amount
was falling down as we from April(PAY_AMT6) to September(PAY_AMT1).

We can conclude from the above plot that PAY_AMT1 variable is a key variable which finally helps us to understand the payment behaviour and gives us insight whether the customer will be defaulter or not.

##Bill Amount
"""

bill_amt_columns = PayColumnsNames('BILL_AMT')
figure, ax = plt.subplots(2,3)
figure.set_size_inches(18,8)


for i in range(len(pay_columns)):
    row,col = int(i/3), i%3

    ax[row,col].hist(credit_df[bill_amt_columns[i]], 30, color ='green')
    ax[row,col].hist(credit_df[bill_amt_columns[i]][(credit_df['defaulter']==1)],30,rwidth=0.9,color='red',alpha = 0.7)
    ax[row,col].set_title(bill_amt_columns[i])
    #adding scaling to make the graph more helpful
    ax[row,col].set_yscale('log', nonposy='clip')


    
plt.tight_layout()
plt.show()

"""**The above histogram shows the distribution of Bill amount generated for each month explicitly for defaulters**

The green bins in the histograms shows the bill_amount for all the customers from September to April Month.

The red bins in the histogram tells the payment amount of customers who were actually a defaulter from September to April Month.

Above plot indicates that there is higher proportion of clients for whom the bill amount is high but payment done against the same is very low. 

From the above plot , we can say that bill amount variable based on different months from April to September not able to give us a clear insight from the plot whether the customer will be defaulter or not compared to payment amount variable.

Hence payment amount features are more significant variables compared to the bill amount features.

##**BI-VARIATE ANALYSIS**

##Defaulter vs Sex
"""

# Checking the number of counts of defaulters and non defaulters sex-wise


sns.countplot(x='SEX', data=credit_df,hue="defaulter", palette="muted")

credit_df.loc[credit_df['SEX']==2,'defaulter'].value_counts()

"""**It is evident from the above count plot output that males have overall less default payment rate w.r.t females**

**Both in Defaulter and Non-Defaults count, females were having higher proportion  (Sex=2)**

##Defaulter vs Education
"""

# Checking the number of counts of defaulters and non defaulters education-wise


sns.countplot(x='EDUCATION', data=credit_df,hue="defaulter", palette="muted")

credit_df.loc[credit_df['EDUCATION']==2,'defaulter'].value_counts()

"""**The credit card holders with a university degree were the customers with the highest number of default payment rate compared to other degree holders.**

##Defaulter vs Marriage
"""

# Checking the number of counts of defaulters and non defaulters marriage-wise


sns.countplot(x='MARRIAGE', data=credit_df,hue="defaulter", palette="muted")

credit_df.loc[credit_df['MARRIAGE']==2,'defaulter'].value_counts()

"""**It is evident from the above plot that both the credit card holders who were singles and married used to do default in payments.**

##Defaulter vs AGE
"""

figure, ax = plt.subplots( figsize = (25,15))
sns.countplot(x='AGE', data=credit_df,hue="defaulter", palette="muted") 
plt.title('Distribution of Age')

"""**We can say from the above graph that the young people between the age of 23 and 31 mostly default the credit card payment compared to older people.we can also see a similar trend of high default rate after the age of 60 in senior citizens as well.**

##**Multi-variate Analysis**

##SEX VS CREDIT LIMIT VS EDUCATION
"""

plt.figure(figsize=(25,9))
sns.barplot(x='SEX',y='LIMIT_BAL',data=credit_df,hue='EDUCATION')
plt.title('Multivariate Analysis(Sex/Limit_Bal/Education')

"""**The above figure tells us that  the highest LIMIT_BAL/credit limit amount is given to the graduate education credit card holders in both the sex.**

**On the contrary, the least credit limit amount is given to the high school education credit card holders in both the sex.**

"""

g = sns.FacetGrid(credit_df, row='defaulter', col='SEX')
g = g.map(plt.hist, 'AGE')

"""It can be seen that females of age group 20-30 have very high tendency to default payment compared to males in all age brackets. Hence we can keep the SEX column of clients to predict probability of defaulting payment.

# **Heat Map**
"""

plt.subplots(figsize=(30,20))
sns.heatmap(credit_df.corr(), annot=True)
plt.show()

"""## The above heatmap clearly tells you that the PAY_1 to PAY_6 variables are the strongest predictors of defaulter which are postively correlated with the target variable.

## Hence we can conclude from the above heatmap that the most important feature infulencing our target feature will be Pay_1 to Pay_6. But there is multicollinearity between the Payment Repayment Status features.

## The LIMIT_BAL and PAY_AMT_1 to PAY_AMT_6 variables showing a negative correlation with the target variable defaulter

## Apart from that,we can find a positive correlation among the features LIMIT_BAL and BILL_AMT

## A negative correlation between AGE and MARRIAGE

## We will first train model wil all the features and try reducing the non-important features.

#**Outliers**
"""

# find Numerical variables
numerical = [var for var in credit_df.columns if credit_df[var].dtype !='O']
print('There are {} numerical variables'.format(len(numerical)))

discrete = []
for var in numerical:
    if len(credit_df[var].unique())<20:
        print(var, ' values: ', credit_df[var].unique())
        discrete.append(var)
        
print('There are {} discrete variables'.format(len(discrete)))

# outliers in discrete variables
for var in discrete:
    print(credit_df[var].value_counts() / np.float(len(credit_df)))
    print()

"""**Plotting outliers in discrete variables**"""

sns.boxplot(x='defaulter',y='AGE',data=credit_df,palette='rainbow')

sns.boxplot(x='defaulter',y='EDUCATION',data=credit_df,palette='rainbow')

sns.boxplot(x='defaulter',y='MARRIAGE',data=credit_df,palette='rainbow')

sns.boxplot(x='defaulter',y='SEX',data=credit_df,palette='rainbow')

sns.boxplot(x='SEX',hue='defaulter', y='AGE',data=credit_df,palette="rainbow")

sns.boxplot(x='MARRIAGE',hue='SEX', y='AGE',data=credit_df,palette="rainbow" )

"""Marriage, age, and sex. The dataset mostly contains couples in their mid-30s to mid-40s and single people in their mid-20s to early-30s."""

sns.boxplot(x='MARRIAGE',hue='defaulter', y='AGE',data=credit_df,palette="rainbow")

"""**Plotting outliers in continuous variables**"""

continuous = [var for var in numerical if var not in discrete ]

for var in continuous:
    plt.figure(figsize=(15,6))
    plt.subplot(1, 2, 1)
    fig = sns.boxplot(y=credit_df[var])
    fig.set_title('')
    fig.set_ylabel(var)
    
    plt.subplot(1, 2, 2)
    fig = sns.distplot(credit_df[var].dropna())
    fig.set_ylabel('count')
    fig.set_xlabel(var)

    plt.show()

"""Outliers can be visualised as the dots outside the whiskers in the boxplots. The majority of the continuous and discrete variables seem to contain outliers. In addition, the majority of the variables are not normally distributed.

We are not removing any of the outliers as of now because it may lead to information loss since our dataset is small and our model will not be able to train well if we remove few records due to outliers . 

It's not always adviced to remove outliers inorder to increase the accuracy of the model,here our data records are compartively low in number.Hence we have decided not to remove the outliers as of now

#**Binning (AGE)**
"""

# Bin ‘AGE’ data to 6 groups
bins= [21,30,40,50,60,70,80]
labels = list(range(6))
credit_df['AGE'] = pd.cut(credit_df['AGE'],bins=bins, labels=labels,right=False)

from sklearn.preprocessing import LabelEncoder
# creating instance of labelencoder
labelencoder = LabelEncoder()
# Assigning numerical values and storing in another column
credit_df['AGE_Encoded'] = labelencoder.fit_transform(credit_df['AGE'])
credit_df['AGE_Encoded'].value_counts()

##droping the old age column
credit_df = credit_df.drop('AGE',axis=1)

credit_df.columns

"""# **One Hot Encoding**"""

credit_df = pd.get_dummies(credit_df, columns = ['EDUCATION', 'MARRIAGE','SEX','PAY_1', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6'])

credit_df.columns

"""#**Data Preparation for Model Buliding**





"""

# created a copy of our dataset for modeling
credit_card_df=credit_df.copy()

#Initially we have decided to train our baseline model with all features
X = credit_card_df.drop(columns=['defaulter']) #Independent features
y = credit_card_df['defaulter'] #Dependent features

features = ['LIMIT_BAL', 'BILL_AMT1', 'BILL_AMT2',
       'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT1',
       'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6',
       'Total_Bill_AMT', 'Total_Paid_AMT', 'Pending_Payment_AMT','AGE_Encoded']

"""#**Handling Imbalance Target Variable** (SMOTE)

###Imbalanced dataset is a type of dataset where the distribution of labels across the dataset is not balanced i.e. the distribution is biased or skewed. Group having more data points/samples is known as majority class where the group having less data points is known as minority class.

###Credit card default prediction (defaulters are very few among all financial transaction)

###In such cases, minority class is more important than the majority class and the motive of classifier is to effectively classify the minority class from the majority class e.g. identify default transaction from all transactions. Such high imbalanced distribution pose a challenge for class prediction.

###There are many techniques available to handle class imbalance. One of the popular techniques is up-sampling (e.g. SMOTE) in which more similar data points are added to minority class to make class distribution equal. On this up-sampled modified data, any classifier can be applied.

###Handling the imbalance is really a crucial step to carried out.We have used S.M.O.T.E Technique to solve this problem
"""

from imblearn.over_sampling import SMOTE

smote = SMOTE()

# fit predictor and target variable
x_smote, y_smote = smote.fit_resample(X,y)

print('Original dataset shape', len(credit_card_df))
print('Resampled dataset shape', len(y_smote))

y_smote.value_counts()

"""#**Spliting the data into train and test** (70% train,30% test)"""

# Splitting the dataset into the Training set and Test set
X_train, X_test, y_train, y_test = train_test_split(x_smote, y_smote, test_size = 0.3, random_state = 42)

X_train.shape ,X_test.shape

"""#**Scaling the Independent features** (Standard Scalar)"""

sc= StandardScaler()
X_train[features]=sc.fit_transform(X_train[features]) # fit on training data columns and transform the training data columns
X_test[features]=sc.transform(X_test[features]) # transform the testing data columns

"""#**Model buliding** (Baseline model with default parameters)

##Comparing different models using a function model_score and finally selecting the best model after Optimization
"""

from pandas.core.common import random_state
def model_score(model_name, model, X_train,
                X_test, y_train, y_test, test=False):
    

    my_model = model
    my_model.fit(X_train,y_train)
    
    print(f'{model_name}  training accuracy score: {my_model.score(X_train,y_train):.4}\n')
    print(f'{model_name} test accuracy score: {my_model.score(X_test,y_test):.4}\n')
    
    
    return my_model
  
lr = model_score('LogReg', LogisticRegression(random_state=42),
                       X_train, X_test,
                       y_train, y_test,test=False)

svc = model_score('SVM', SVC(random_state=42),
                       X_train, X_test,
                       y_train,y_test,test=False)

knn = model_score('KNN', KNeighborsClassifier(),
                       X_train,X_test,
                       y_train,y_test,test=False)

gbm = model_score('XGBoost', XGBClassifier(random_state=42),
                       X_train,X_test,
                       y_train,y_test,test=False)


rf = model_score('RF', RandomForestClassifier(random_state=42),
                       X_train,X_test,
                       y_train,y_test,test=False)

nb = model_score('NB',GaussianNB(),
                       X_train, X_test,
                       y_train,y_test,test=False)

"""## The Random Forest and KNN Model are actually overfitting with high train accuracy(low Bias) and Low Test Accuracy(High Variance) while we take into consideration default parameters of the respective models.


## The XGBoostModel,S.V.M and Logistic Regression Models provide us with good train and test accuracy.

## The Gaussian Naive Bayes Model gave us the lowest train and test accuracy.

#**Optimization**

###What are we optimizing for?

###Using accuracy score as a evaluation metrics for such highly imbalanced dataset is not a good measure of classifier performance.

###In such cases, evaluation metrics like ROC-AUC curve are a good indicator of classifier performance. It is a measure of how good model is at distinguishing between various class. Higher the ROC-AUC score, better the model is at predicting 0s as 0s and 1s as 1s. Just to remind, ROC is a probability curve and AUC represents degree or measure of separability. Apart from this metric, we will also check on recall score, false-positive (FP) and false-negative (FN) score as we build our classifier



###**Ideally, we do not want to miss any potentially defaults to fall through the cracks, so our optimal model will minimize False Negatives (optimize Recall Score)**

## **Optimizing the Recall score**



### Our aim will be to reduce the false negative count because it will enhance the recall evaluation metric of our model. Since our problem statement is to predict the credit card defaulter,hence recall is our key parameter to focus among  the evaluation metrics.

### The optimization of recall value basically  help us in not missing any credit card defaulter from our analaysis by reducing the false negative rate in our model.

###Inorder to optimize our model recall,follows below steps-



## 1. Feature Selection/ Feature Importance

## 2. Hyperparameteric Tunning using Grid SearchCV.

#**Evaluation Metrics**

##**Logistic Regression(Default Parameters)**

###Classification Report
"""

y_pred_lr = lr.predict(X_test)

print(classification_report(y_test, y_pred_lr))

"""###Confusion Matrix"""

cm = confusion_matrix(y_test, y_pred_lr)
print(cm)

sns.heatmap(cm,annot= True,linewidths=1,cmap= 'coolwarm')

from sklearn.metrics import  accuracy_score, f1_score, precision_score, recall_score, roc_auc_score
roc=roc_auc_score(y_test, y_pred_lr)
acc = accuracy_score(y_test, y_pred_lr)
prec = precision_score(y_test, y_pred_lr)
rec = recall_score(y_test, y_pred_lr)
f1 = f1_score(y_test, y_pred_lr)

model_result = pd.DataFrame([['Logistic Regression', acc,prec,rec, f1,roc]],
               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score','ROC'])

model_result

"""#**Hyperparameter Tuning using GridSearchCV**

##**Logistic Regression**
"""

#Parameters to be used
penalty = ['l1', 'l2','elasticnet']
c_values = [100, 10, 1.0, 0.1, 0.01,0.001]
class_weight = [{0:10, 1:15}, 'balanced']
solver = ['saga']
grid = dict(penalty=penalty,C=c_values, class_weight=class_weight)

lrm = LogisticRegression(random_state=42)
grid_lr = GridSearchCV(estimator=lr, param_grid=grid,verbose=2, n_jobs=-1, cv=3, scoring='recall')

grid_lr.fit(X_train,y_train)

grid_lr.best_params_

"""###C is known as a "hyperparameter." The parameters are numbers that tell the model what to do with the characteristics, whereas the hyperparameters instruct the model on how to choose parameters.
###Regularization will penalize the extreme parameters, the extreme values in the training data lead to overfitting.
###A high value of C tells the model to give more weight to the training data. A lower value of C will indicate the model gives complexity more weight at the cost of fitting the data. Thus, a high Hyper Parameter value C indicates that training data is more important and reflects the real-world data, whereas a low value is just the opposite of this

#**Evaluation Metrics**

##**Logistic Regression**(After Hyperparameter tuning)

###Check for Overfit
"""

y_pred_lr1= grid_lr.predict(X_test)  # predicting on test data
y_pred_train_lr1 = grid_lr.predict(X_train) # predicting on train data

print(f'Train Accuracy: {accuracy_score(y_train,y_pred_train_lr1)}')
print(f'Test Accuracy: {accuracy_score(y_test, y_pred_lr1)}')
print(f'Area Under Curve: {roc_auc_score(y_test, y_pred_lr1)}')

"""###Classification Report"""

print(classification_report(y_test, y_pred_lr1))

"""###Confusion Matrix"""

cm=confusion_matrix(y_test, y_pred_lr1)
print(cm)

sns.heatmap(cm,annot= True,linewidths=1,cmap= 'coolwarm')

roc=roc_auc_score(y_test, y_pred_lr1)
acc = accuracy_score(y_test, y_pred_lr1)
prec = precision_score(y_test, y_pred_lr1)
rec = recall_score(y_test, y_pred_lr1)
f1 = f1_score(y_test, y_pred_lr1)

model = pd.DataFrame([['Logistic Regression Tuned', acc,prec,rec, f1,roc]],
               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score','ROC'])

model_result = model_result.append(model, ignore_index = True)
model_result

"""#**Evaluation Metrics**

##**Support Vector Machine(Default Parameters)**

###Check for Overfit
"""

y_pred_test_svm = svc.predict(X_test)
y_pred_train_svm = svc.predict(X_train)
print(f'Train Accuracy: {accuracy_score(y_train, y_pred_train_svm)}')
print(f'Test Accuracy: {accuracy_score(y_test, y_pred_test_svm)}')
print(f'Area Under Curve: {roc_auc_score(y_test, y_pred_test_svm)}')

"""###Classification Report"""

print(classification_report(y_test, y_pred_test_svm))

"""###Confusion Matrix"""

cm = confusion_matrix(y_test, y_pred_test_svm)
print(cm)

sns.heatmap(cm,annot= True,linewidths=1,cmap= 'coolwarm')

from sklearn.metrics import  accuracy_score, f1_score, precision_score, recall_score, roc_auc_score
roc=roc_auc_score(y_test,y_pred_test_svm)
acc = accuracy_score(y_test, y_pred_test_svm)
prec = precision_score(y_test,y_pred_test_svm)
rec = recall_score(y_test, y_pred_test_svm)
f1 = f1_score(y_test, y_pred_test_svm)

model = pd.DataFrame([['Support Vector Machine', acc,prec,rec, f1,roc]],
               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score','ROC'])

model_result = model_result.append(model, ignore_index = True)
model_result

"""#**Hyperparameter Tuning using GridSearchCV**

##**Support vector machine**
"""

svc_params = [{'gamma':[0.001,0.01,1],'kernel':['rbf'],'C':[1,10,100]}]
svc = SVC(random_state=42)

"""### 0.0001 < gamma < 10  (0.001, 0.01, 0.1, 1, 10, 100)    
### 0.1 < C < 100 (0.001, 0.01, 0.1, 1, 10, 100,1000)
"""

gridCV= GridSearchCV(estimator= svc, param_grid=svc_params, scoring='recall',cv=3, verbose = 3)
grid_svm =gridCV.fit(X_train,y_train)

print("Best Parameters:",grid_svm.best_params_)

print("Train Score:",grid_svm.best_score_)

print("Test Score:",grid_svm.score(X_test,y_test))

"""#**Evaluation Metrics**

##**Support Vector Machine** (After Hyperparmeter tuning)

###Check for Overfit
"""

y_pred_test_svm1 = grid_svm.predict(X_test)
y_pred_train_svm1 = grid_svm.predict(X_train)
print(f'Train Accuracy: {accuracy_score(y_train, y_pred_train_svm1)}')
print(f'Test Accuracy: {accuracy_score(y_test, y_pred_test_svm1)}')
print(f'Area Under Curve: {roc_auc_score(y_test, y_pred_test_svm1)}')

"""###Classification Report"""

print(classification_report(y_test,y_pred_test_svm1))

"""###Confusion Matrix"""

cm = confusion_matrix(y_test, y_pred_test_svm1)
print(cm)

sns.heatmap(cm,annot= True,linewidths=1,cmap= 'coolwarm')

roc=roc_auc_score(y_test, y_pred_test_svm1)
acc = accuracy_score(y_test, y_pred_test_svm1)
prec = precision_score(y_test, y_pred_test_svm1)
rec = recall_score(y_test, y_pred_test_svm1)
f1 = f1_score(y_test, y_pred_test_svm1)
model =  pd.DataFrame([['Support Vector Machine  Tuned', acc,prec,rec, f1,roc]],
               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score','ROC'])
model_result = model_result.append(model, ignore_index = True)
model_result

"""# **Evaluation Metrics**

##**K- Nearest Neighbour(Default Parameters)**

###Classification Report
"""

y_pred_knn = knn.predict(X_test)

print(classification_report(y_test, y_pred_knn))

"""###Confusion Matrix"""

cm = confusion_matrix(y_test, y_pred_knn)
print(cm)

sns.heatmap(cm,annot= True,linewidths=1,cmap= 'coolwarm')

from sklearn.metrics import  accuracy_score, f1_score, precision_score, recall_score, roc_auc_score
roc=roc_auc_score(y_test, y_pred_knn)
acc = accuracy_score(y_test, y_pred_knn)
prec = precision_score(y_test, y_pred_knn)
rec = recall_score(y_test, y_pred_knn)
f1 = f1_score(y_test, y_pred_knn)

model = pd.DataFrame([['K-Nearest Neighbour', acc,prec,rec, f1,roc]],
               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score','ROC'])

model_result = model_result.append(model, ignore_index = True)
model_result

"""#**Hyperparameter Tuning using GridSearchCV**

##  **K-Nearest Neighbour**
"""

error_rate = []

for i in range(1,30):
    
    knn = KNeighborsClassifier(n_neighbors=i,n_jobs=-1)
    knn.fit(X_train,y_train)
    pred_i = knn.predict(X_test)
    error_rate.append(np.mean(pred_i != y_test))

plt.figure(figsize=(10,6))
plt.plot(range(1,30),error_rate,color='blue', linestyle='dashed', marker='o',
         markerfacecolor='red', markersize=10)
plt.title('Error Rate vs. K Value')
plt.xlabel('K')
plt.ylabel('Error Rate')

from sklearn.neighbors import KNeighborsClassifier 
knn = KNeighborsClassifier()

k_range =  [int(x) for x in np.linspace(14, 20,6)] #if the k-value is below 18 then it was giving us an overfit model ,hence we have taken this range of k value for tunning
weight_options = ['uniform']
knn_param = {'n_neighbors': k_range, 'weights': weight_options}

grid_search = GridSearchCV(estimator = knn,param_grid = knn_param,scoring='recall',cv=3,verbose=2)   #59 mins
grid_knn = grid_search.fit(X_train,y_train)

best_accuracy_knn = grid_knn.best_score_
print('Accuracy on Cross Validation set :',best_accuracy_knn)

best_parameters_knn = grid_knn.best_params_
best_parameters_knn

#Checking  the best estimator
grid_knn.best_estimator_

"""# **Evaluation Metrics**

##**K- Nearest Neighbour**(After Hyperparameter tuning)

###Check for Overfit
"""

y_pred_test_knn = grid_knn.predict(X_test)
y_pred_train_knn = grid_knn.predict(X_train)
print(f'Train Accuracy: {accuracy_score(y_train, y_pred_train_knn)}')
print(f'Test Accuracy: {accuracy_score(y_test, y_pred_test_knn)}')
print(f'Area Under Curve: {roc_auc_score(y_test, y_pred_test_knn)}')

"""At low value of K our model was getting overfit,hence as we increased the k value the overfitting problem has been handled and using hypeparameter tuning we get the optimal value of k as 15

###Classification Report
"""

print(classification_report(y_test, y_pred_test_knn))

"""###Confusion Matrix"""

cm = confusion_matrix(y_test, y_pred_test_knn)
print(cm)

sns.heatmap(cm,annot= True,linewidths=1,cmap= 'coolwarm')

roc=roc_auc_score(y_test, y_pred_test_knn)
acc = accuracy_score(y_test, y_pred_test_knn)
prec = precision_score(y_test, y_pred_test_knn)
rec = recall_score(y_test, y_pred_test_knn)
f1 = f1_score(y_test, y_pred_test_knn)
model =  pd.DataFrame([['K-Nearest Neighbour Model Tuned', acc,prec,rec, f1,roc]],
               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score','ROC'])
model_result = model_result.append(model, ignore_index = True)
model_result

"""#**Evaluation Metrics**

##**XGBoost(Default Parameters)**

### Classification Report
"""

y_pred_gbm = gbm.predict(X_test)
print(classification_report(y_test, y_pred_gbm))

"""## Confusion Matrix"""

y_pred_gbm = gbm.predict(X_test)
cm = confusion_matrix(y_test, y_pred_gbm)
print(cm)

sns.heatmap(cm,annot= True,linewidths=1,cmap= 'coolwarm')

from sklearn.metrics import  accuracy_score, f1_score, precision_score, recall_score, roc_auc_score
roc=roc_auc_score(y_test, y_pred_gbm)
acc = accuracy_score(y_test, y_pred_gbm)
prec = precision_score(y_test, y_pred_gbm)
rec = recall_score(y_test, y_pred_gbm)
f1 = f1_score(y_test, y_pred_gbm)

model = pd.DataFrame([['XGBoost', acc,prec,rec, f1,roc]],
               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score','ROC'])

model_result = model_result.append(model, ignore_index = True)
model_result

"""#**Hyperparameter Tuning using Randomized and GridSearchCV**

##**XGBoost**

###RandomizedSearchCV
"""

from sklearn.model_selection import RandomizedSearchCV

#Parameters
n_estimators = [int(x) for x in np.linspace(10, 200, 10)]
max_features = ['auto', 'sqrt','log2']
max_depth = [int(x) for x in np.linspace(1, 10,10)]
min_child_weight = [2, 5, 10,15,20]
learning_rate = [0.0001,0.01,0.1,0.3,0.6]
gamma = [1,5,10,15,20]
reg_alpha = [0.1,0.2,0.3,0.5,0.6,0.8,1.0]
# Create the random grid
xgb_grid = {'n_estimators': n_estimators, 'max_features': max_features, 
               'max_depth': max_depth,'gamma': gamma,
             'learning_rate':learning_rate,
               'min_child_weight' : min_child_weight,
               'reg_alpha': reg_alpha }

xgb=XGBClassifier(random_state=42)
xgb_randomcv=RandomizedSearchCV(estimator=xgb,param_distributions=xgb_grid,n_iter=30,cv=3,scoring='recall',verbose=2,
                               )
xgb_randomcv.fit(X_train,y_train)

#Checking the best parameters
xgb_randomcv.best_params_

#Checking  the best estimator
xgb_randomcv.best_estimator_

best_param_grid=xgb_randomcv.best_estimator_
y_pred=best_param_grid.predict(X_test)

"""###GridSearchCv"""

param_grid_ = {
    'learning_rate': [xgb_randomcv.best_params_['learning_rate']],
    'max_depth': [xgb_randomcv.best_params_['max_depth']] ,
    'reg_alpha': [xgb_randomcv.best_params_['reg_alpha']],
    'max_features': [xgb_randomcv.best_params_['max_features']],
    'min_child_weight': [xgb_randomcv.best_params_['min_child_weight'], 
                         xgb_randomcv.best_params_['min_child_weight']+2, 
                         xgb_randomcv.best_params_['min_child_weight'] + 4],
    'gamma': [xgb_randomcv.best_params_['gamma'] - 2,
                         xgb_randomcv.best_params_['gamma'], 
                         xgb_randomcv.best_params_['gamma'] + 2], 
                          
                          
    'n_estimators': [xgb_randomcv.best_params_['n_estimators'] - 50, xgb_randomcv.best_params_['n_estimators'] - 25, 
                     xgb_randomcv.best_params_['n_estimators'], 
                     xgb_randomcv.best_params_['n_estimators'] + 25, xgb_randomcv.best_params_['n_estimators'] + 50]
}

print(param_grid_)

grid_search_xgb=GridSearchCV(estimator=xgb,param_grid=param_grid_,cv=3,scoring='recall', verbose=2)
grid_search_xgb.fit(X_train,y_train)

#Checking for best estimator
grid_search_xgb.best_estimator_

best_grid1=grid_search_xgb.best_estimator_

best_score_xgb = grid_search_xgb.best_score_
print('Scoring on Cross Validation set :',best_score_xgb)

"""#**Evaluation Metrics**

##**XGBoost** (After Hyperparameter Tuning)

##Check for Overfit
"""

y_pred_test_xgb = grid_search_xgb.predict(X_test)  # predicting on test data
y_pred_train_xgb = grid_search_xgb.predict(X_train) # predicting on train data
print('Accuracy on Training  and Test Data :')

print(accuracy_score(y_train,y_pred_train_xgb))

print(accuracy_score(y_test, y_pred_test_xgb))

"""###Classification Report"""

print((classification_report(y_test, y_pred_test_xgb)))

"""###Confusion Matrix"""

cm = confusion_matrix(y_test, y_pred_test_xgb)
print(cm)

sns.heatmap(cm,annot= True,linewidths=1,cmap= 'coolwarm')

roc=roc_auc_score(y_test, y_pred_test_xgb)
acc = accuracy_score(y_test, y_pred_test_xgb)
prec = precision_score(y_test,y_pred_test_xgb)
rec = recall_score(y_test,y_pred_test_xgb)
f1 = f1_score(y_test,y_pred_test_xgb)
model =  pd.DataFrame([['XGBOOST Tuned', acc,prec,rec, f1,roc]],
               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score','ROC'])
model_result = model_result.append(model, ignore_index = True)
model_result

"""#**Feature Selection using Embedded Method**

##**XGBoost**

###These methods encompass the benefits of both the wrapper and filter methods, by including interactions of features but also maintaining reasonable computational cost. Embedded methods are iterative in the sense that takes care of each iteration of the model training process and carefully extracts those features which contribute the most to the training for a particular iteration.

###There are many feature selection scores one could perform to determine which features are most useful. For this case, we will use Feature Importance.

###Feature Importance is the process of assigning scores to each feature, depending on how useful it is in predicting the target variable
"""

temp = pd.DataFrame({'Feature': list(x_smote.columns), 'Feature Importance': gbm.feature_importances_})
temp = temp.sort_values(by="Feature Importance", ascending=False)

plt.figure(figsize=(15,10))
plt.title('Feature Importance XGBoost Model', fontsize=14)
s=sns.barplot(x='Feature', y='Feature Importance', data=temp)
s.set_xticklabels(s.get_xticklabels(), rotation=90)
plt.show();

"""#**Evaluation Metrics**

## **Random Forest(Default Parameters)**
"""

y_pred_test_rf = rf.predict(X_test)
y_pred_train_rf = rf.predict(X_train)
print(accuracy_score(y_train, y_pred_train_rf))
print(accuracy_score(y_test, y_pred_test_rf))

"""###Classification Report"""

print(classification_report(y_test, y_pred_test_rf))

"""###Confusion Matrix"""

cm = confusion_matrix(y_test, y_pred_test_rf)
print(cm)

sns.heatmap(cm,annot= True,linewidths=1,cmap= 'coolwarm')

from sklearn.metrics import  accuracy_score, f1_score, precision_score, recall_score, roc_auc_score
roc=roc_auc_score(y_test,y_pred_test_rf)
acc = accuracy_score(y_test, y_pred_test_rf)
prec = precision_score(y_test,y_pred_test_rf)
rec = recall_score(y_test, y_pred_test_rf)
f1 = f1_score(y_test, y_pred_test_rf)

model = pd.DataFrame([['Random Forest', acc,prec,rec, f1,roc]],
               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score','ROC'])

model_result = model_result.append(model, ignore_index = True)
model_result

"""#**Hyperparameter Tuning using Randomized and GridSearchCV**

##**Random Forest**

###RandomisedsearchCv
"""

#Parameters
n_estimators = [int(x) for x in np.linspace(100, 1000, 10)]
max_features = ['auto', 0.4,'log2']
max_depth = [int(x) for x in np.linspace(1, 10,10)]
min_samples_split = [2, 5, 10,14]
min_samples_leaf = [2, 4, 6]
# Create the random grid
random_grid = {'n_estimators': n_estimators, 'max_features': max_features, 
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf,
              'criterion':['gini']}
print(random_grid)

rf1=RandomForestClassifier()
rf_randomcv=RandomizedSearchCV(estimator=rf1,param_distributions=random_grid,n_iter=30,cv=3,scoring='recall',verbose=2,
                               random_state=10)
rf_randomcv.fit(X_train, y_train)

#Checking the best parameters
rf_randomcv.best_params_

best_random_grid=rf_randomcv.best_estimator_
y_pred=best_random_grid.predict( X_test)

"""###GridSearchCv"""

param_grid = {
    'criterion': [rf_randomcv.best_params_['criterion']],

    'max_depth': [rf_randomcv.best_params_['max_depth'],rf_randomcv.best_params_['max_depth']-2,rf_randomcv.best_params_['max_depth']-5],

    'max_features': [rf_randomcv.best_params_['max_features']],

    'min_samples_leaf': [rf_randomcv.best_params_['min_samples_leaf'], 
                         rf_randomcv.best_params_['min_samples_leaf']-1, 
                         rf_randomcv.best_params_['min_samples_leaf'] -2],
              
    'min_samples_split': [ rf_randomcv.best_params_['min_samples_split'] + 1,
                          rf_randomcv.best_params_['min_samples_split']],
                          
    'n_estimators': [ 
                     rf_randomcv.best_params_['n_estimators']+600, 
                     rf_randomcv.best_params_['n_estimators']+500 ]
            }

print(param_grid)

grid_search=GridSearchCV(estimator=rf1,param_grid=param_grid,cv=3,scoring='recall', verbose=2)
grid_search.fit(X_train, y_train)

#Checking for best estimator
grid_search.best_estimator_

best_grid=grid_search.best_estimator_

best_accuracy_rf = grid_search.best_score_
print('Accuracy on Training set :',best_accuracy_rf)

"""#**Evaluation Metrics**

## **Random Forest** (After Hyperparameter tuning)
"""

y_pred_train_rf1 = grid_search.predict(X_train)
y_pred_test_rf1=grid_search.predict(X_test)
print(accuracy_score(y_train, y_pred_train_rf1))
print(accuracy_score(y_test, y_pred_test_rf1))

"""###Classification Report"""

print(classification_report(y_test, y_pred_test_rf1))

"""###Confusion Matrix"""

cm = confusion_matrix(y_test, y_pred_test_rf1)
print(cm)

roc=roc_auc_score(y_test, y_pred_test_rf1)
acc = accuracy_score(y_test,  y_pred_test_rf1)
prec = precision_score(y_test,  y_pred_test_rf1)
rec = recall_score(y_test,  y_pred_test_rf1)
f1 = f1_score(y_test,  y_pred_test_rf1)
model =  pd.DataFrame([['Random Forest Classifier Tuned', acc,prec,rec, f1,roc]],
               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score','ROC'])

model_result = model_result.append(model, ignore_index = True)
model_result

"""#**Feature Selection using Embedded Method**

##**Random Forest**
"""

temp = pd.DataFrame({'Feature': list(x_smote.columns), 'Feature Importance': rf.feature_importances_})
temp = temp.sort_values(by="Feature Importance", ascending=False)

plt.figure(figsize=(19,10))
plt.title('Feature Importance Random Forest Model', fontsize=14)
s=sns.barplot(x='Feature', y='Feature Importance', data=temp)
s.set_xticklabels(s.get_xticklabels(), rotation=90)
plt.show();



"""#**Evaluation Metrics**

## **Naive Bayes**
"""

y_pred_test_nb = nb.predict(X_test)
y_pred_train_nb = nb.predict(X_train)
print(accuracy_score(y_train, y_pred_train_nb))
print(accuracy_score(y_test, y_pred_test_nb))

"""###Classification Report"""

print(classification_report(y_test, y_pred_test_nb))

"""###Confusion Matrix"""

cm = confusion_matrix(y_test, y_pred_test_nb)
print(cm)

sns.heatmap(cm,annot= True,linewidths=1,cmap= 'coolwarm')

from sklearn.metrics import  accuracy_score, f1_score, precision_score, recall_score, roc_auc_score
roc=roc_auc_score(y_test,y_pred_test_nb)
acc = accuracy_score(y_test, y_pred_test_nb)
prec = precision_score(y_test,y_pred_test_nb)
rec = recall_score(y_test, y_pred_test_nb)
f1 = f1_score(y_test, y_pred_test_nb)

model = pd.DataFrame([['Naive Bayes', acc,prec,rec, f1,roc]],
               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score','ROC'])

model_result = model_result.append(model, ignore_index = True)
model_result

"""#**ROC Curve**

###Receiver Operating Characteristic(ROC) summarizes the model’s performance by evaluating the trade offs between true positive rate (sensitivity) and false positive rate(1- specificity). For plotting ROC, it is advisable to assume p > 0.5 since we are more concerned about success rate.

###ROC summarizes the predictive power for all possible values of p > 0.5. The area under curve (AUC), referred to as index of accuracy(A) or concordance index, is a perfect performance metric for ROC curve. Higher the area under curve, better the prediction power of the model
"""

print("ROC AUC score Random Forest= ", roc_auc_score(y_test, grid_search.predict(X_test)))
print("ROC AUC score XGBoost = ", roc_auc_score(y_test, grid_search_xgb.predict(X_test)))
print("ROC AUC score KNN = ", roc_auc_score(y_test, grid_knn.predict(X_test)))
print("ROC AUC score Logistic= ", roc_auc_score(y_test, grid_lr.predict(X_test)))
print("ROC AUC score SVM= ", roc_auc_score(y_test, grid_svm.predict(X_test)))
print("ROC AUC score NB= ", roc_auc_score(y_test, nb.predict(X_test)))

from sklearn import metrics

# false positive rate,fpr= FP/(TN+FP) OR fpr=1-specificty, tpr=sensitivity 
y_pred_log_p =grid_lr.predict(X_test)
y_pred_svc_p =grid_svm.predict(X_test)
y_pred_knn_p =grid_knn.predict(X_test)
y_pred_rf_p =grid_search.predict(X_test)
y_pred_xgb_p =grid_search_xgb.predict(X_test)
y_pred_nb_p =nb.predict(X_test)

model = [grid_lr,grid_svm,grid_knn,grid_search,grid_search_xgb,nb]

models=[y_pred_log_p,y_pred_svc_p,y_pred_knn_p,y_pred_rf_p,y_pred_xgb_p,y_pred_nb_p]
label=['Logistic','SVM','KNN','Random Forest','XGBoost','Naive Bayes']

# plotting ROC curves
plt.figure(figsize=(20, 15))
m= [0,1,2,3,4,5]
for m in m:
    fpr, tpr,thresholds= metrics.roc_curve(y_test,models[m])
    auc = metrics.roc_auc_score(y_test,model[m].predict(X_test))
    plt.plot(fpr, tpr, label='%s ROC (area = %0.3f)' % (label[m], auc))
plt.plot([0, 1], [0, 1],c='violet',ls='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('1-Specificity(False Positive Rate)')
plt.ylabel('Sensitivity(True Positive Rate)')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()

"""#**Conclusion**

###1. **The highest proportion of credit card holders were youth in the age of 29,thus we can conclude that mostly credit cards were popular among youths of taiwan than the older people.**



###2. **The Correlation between features and target variable tells us the level of education and financial stability of the customers had high impact on the default rate.**



###3. **The data also conveys us that the best indicator of delinquency is the behavior of the customer, which has been predominantely seen in the past couple of months payment repayment status .The heat map shows us the high correlation of payment repayment status with the target variable.**



###4. **The females of age group 20-30 have very high tendency to default payment compared to males in all age brackets.** 



###5. **Comparatively after hyperparameter tuning the XGBoost Model comes out to be the best model in terms of its AUC_ROC score(0.875) and Recall score(0.82) and we can predict with 87.45% accuracy, whether a customer is likely to default next month.**

The reason is, XGBoost has high predictive power and is almost 10 times faster than the other gradient boosting techniques. It also includes a variety of regularization which reduces overfitting and improves overall performance.



###6. **The Second best model was the Support Vector Machine with a AUC_ROC score of 0.875 and a Recall score of 0.805 and  we can predict with 87.5% accuracy, whether a customer is likely to default next month.**

SVM model perform well when we select the proper kernel and the risk of overfitting is comparatively less.
We have used rbf kernel and after fine tunning the model we got Cost C parameter best value as 100 and gamma as 0.01.



###7. **But it would be worth using Logistic Regression model for production since we do not just need a reliable model with good ROC_AUC Score but also a model that is quick and less complex.**



###8. **Except Naive Bayes model,all the models have got really good ROC_AUC scores with a probability of 0.85 on an average**.



###9.**The Random Forest and KNN models were really overfitting with default parameters and we handle the overfit in both these model by fine tuning the model.**




###10.**Demographics: we see that being Female, More educated, Single and between 30-40years old means a customer is more likely to make payments on time.**
"""

